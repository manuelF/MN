\section{Conclusiones}

Este trabajo nos ha dado la oportunidad de aprender varias lecciones.

\subsection{Herramientas de desarrollo rapido}

El uso de Matlab ha sido fundamental a la hora de entender el problema.
Es muy sencillo perderse en los detalles de la implementaci\'on y no ver que
problema se quiere resolver. Una herramienta din\'amica donde se pueden visualizar
en una pasada las factorizaciones de una matriz, las dimensiones y las cuentas
de forma matricial sin que importen los detalles del calculo.

Ayud\'o tambien que la velocidad de procesamiento de MATLAB es inesperadamente
r\'apida, por lo que se pudo experimentar rapidamente con matrices de covarianza
de pocas y de muchas im\'agenes indistintamente. El costo que se paga por esto es
no saber como lo esta resolviendo por atras y no tener control sobre los datos generados.
En particular, por ejemplo, si usamos la funci\'on \texttt{svd} de MATLAB, se genera
autom\'aticamente las 3 matrices de la factorizaci\'on, pero solamente nos interesa
la $V^t$ (de 784 x 784 elementos). Sin embargo, se genera y guarda tambien la $U$ ( de hasta
60000 x 60000 elementos, ~26 Gb) y $S$ (de hasta 60000 x 784 elementos, ~358Mb), llenando innecesariamente
la mem\'oria RAM, y impidiendonos operar con aun m\'as im\'agenes usando este mecanismo.

\subsection{Importancia de los autovectores}

Leyendo la literatura de m\'etodos computaciones aplicados al algebra lineal, es
imposible evitar toparse constantemente con los autovectores y autovalores. Su
uso atravieza todas las \'areas de los m\'etodos num\'ericos. Son cruciales
en los motores gr\'aficos, a la hora de caracterizar transformaciones t\'ipicas.
Aparecen en las ciencias f\'isicas como formas de resolver problemas de movimiento,
vibraci\'on, y fuerzas mec\'anicas.

El uso dado en este trabajo fue para obtener caraterizaciones de los datos utilizando la
descomposici\'on en autovectores, y de esa manera poder correlacionar distintos puntos que
tengan diferencias dificiles de describir. Aprendimos que la t\'ecnica de descomponer en autovectores
es fundamental para tener en cuenta a la hora de resolver problemas \textit{fuzzy}, donde
el an\'alisis de componentes principales puede traer a luz la verdadera variabilidad de los datos.


\subsection{Mecanismo de reconocimiento}

Las t\'ecnicas que utilizamos para experimentar reconocimiento nos dieron
la pauta de  este es el campo que mas se puede seguir explorando, sea rastreando la literatura
correspondiente como para intentar innovar con alg\'un m\'etodo \'unico e interesante.

Lo que fue realmente notable fue que el uso de ideas que nosotros creiamos m\'as ``inteligentes'',
en realidad resultaron ser peor que la mas \textit{naif}, la distancia euclidia a las medias, para
casi cualquier cantidad de componentes principales.

Esto nos lleva a pensar que el centro de masas de las im\'agenes de entrenamiento seg\'un lo calculamos
nosotros (siguiendo el enunciado y [Sirovich87]) es tal vez una forma \'optima de reconocer
dada la creacion de la matriz de transformacion (la V de covarianza). Si se generase de otra manera esa
transformaci\'on $V^t$, creemos que puede haber m\'etodos diferentes que logren un buen reconocimiento tambi\'en.


\subsection{M\'as puntos de datos, no implican mejores respuestas}

La cantidad de im\'agenes de entrenamiento que deberiamos usar fue un motivo de discusi\'on y hipotesis
dentro del grupo. Claramente quisimos usar todas las que pudieramos, intentando emplear al m\'aximo el
uso del corpus brindado. La contrapartida fue el tiempo de ejecuci\'on. Incluso habiendo implementado
la factorizaci\'on de HouseHolder \'optimo (O($n^3$)), resultaba inaceptable que tardara mas de 24hs
el c\'alculo de los autovectores de la matriz de covarianza con un $\epsilon = 1e-5$. Implementamos
directivas de paralelizaci\'on que pudieran bajar esta complejidad (aunque sea multiplicando por una
constante mas chica). Sin embargo, el tiempo de ejecuci\'on siguio siendo inaceptablemente alto.

Logramos calcular matrices de covarianza hasta para 30.000 im\'agenes de entrenamiento, pero cuando
corrimos los experimentos, el uso de las matrices m\'as ricas no aport\'o casi a un mejor reconocimiento.

Nuestra hipotesis sostiene que esto es asi es porque en el corpus esta bien y apropiadamente
representado cada integrante (d\'igito). Creemos que en problemas de reconocimiento de caras por ejemplo, con
muy pocas muestras de cada integrante, seria escencial tener emplear el corpus al m\'aximo (no necesariamente
usarlo todo, pero seleccionar los integrantes inteligentemente) para poder tener todos los detalles
posibles para hacer un mejor an\'alisis.
