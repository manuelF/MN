\section{Discusi\'on}

\subsection{Tama\~no de los corpus de entrenamiento }
Un punto fundamental del trabajo consisti\'o en el c\'alculo de autovectores de la matriz
de la covarianza de las im\'agenes. En las secciones anteriores se discutio su construcci\'on
y su calculo. Ac\'a comprobamos la relevancia del tama\~no del corpus usado a la hora
de generarla.

Experimentamos tomando 5000, 15000 y 30000 im\'agenes (m\'as no usamos porque era
realmente excesivo el tiempo de c\'alculo en nuestro c\'odigo. Con m\'as tiempo hubi\'eramos podido paralelizar mejor
y poder correr en un cluster o en GPGPU.). 
Como se puede ver en las figuras \ref{fig:HRN2} y \ref{fig:HRVEC},
no hay mejores resultados utilizando matrices de transformaci\'on basadas en menos im\'agenes, por lo que
si ya hemos generado una grande, conviene seguir usandola mientras tengamos que resolver
el mismo problema. Si tenemos que calcularla desde cero, conviene ver cual es el valor que mejora
el tradeoff tiempo/performance, ya que las ganancias de detecci\'on se vuelven marginales.

El \'unico punto delicado en estos casos, de entrenamiento offline, es tener cuidado
de no mezclar datos de entrenamiento con datos de test. Esto podr\'ia ocasionar que
los hitrates sean excesivamente m\'as altos. Es preferible usar matrices generadas
con menos puntos de datos que con m\'as pero ``contaminados''.


\subsection{Iteraciones del c\'alculo de autovectores}
Las matrices de covarianza con las que experimentamos fueron sometidas a
analisis de sus valores bajo la diagonal, para ver si efectivamente estaban
convergiendo. Pudimos ver que si convergian seg\'un
los criterios de suma bajo la diagonal, en figura \ref{fig:SUM}. Para definir
una cota, los valores de la suma esperabamos que se fueran a cero, pero
luego de mas de 500 iteraciones, tendian a un valor cerca de 15. Decidimos
comprobar entonces el promedio de valores bajo la diagonal, en la figura \ref{fig:PROM}.
Ahi pudimos apreciar que eventualmente casi todos los valores eran cero, salvo muy pocos,
que nos estaban forzando a este limite. Decidimos tomar como condicion de corte
que la suma de valores bajo la diagonal fuera $<5000$, lo cual nos dio
alrededor de 100 iteraciones de QR. En efecto estudiamos luego un poco m\'as 
el hitrate, en funci\'on de la cantidad de iteraciones, y pudimos apreciar que
100 iteraciones no variaban significativamente los resultados con respecto a 200 o m\'as.

Un detalle interesante que pudimos observar en las figuras \ref{fig:HR10Neig} a
\ref{fig:HR100Avg} fue que las sucesivas iteraciones del algoritmo QR de
autovectores no presentaba mejorias en la detecci\'on. Esto nos sorprendio
ya que esperabamos que una matriz que convergiera en sus autovectores,
iba a presentar mejor detecci\'on que alguna que estuviera menos refinada.
A\'un mas sorprendete result\'o que la mejor detecci\'on estaba en las primeras 10
iteraciones, no en las ultimas. Sin embargo, decidimos tomar eso como un fen\'omeno
del corpus y nuestro m\'etodo, por lo cual para nuestra experimentacio decidimos
basarnos en la teoria de detecci\'on, que dice que m\'as iteraciones refina mejor. 

Nuestra hipotesis es que aplicar cientos de iteraciones conlleva un error
n\'umerico grande (dado que hay $O(n^3)$ operaciones por iteraci\'on). Esto
hace que se vaya perdiendo precisi\'on numerica y no capte mejor el fen\'omeno.
Lamentablemente no encontramos ninguna referencia a este problema en la literatura,
por lo cual no pudimos corroborar esta hip\'otesis. 


\subsection{Clasificaci\'on de los me\'todos empleados}
Los m\'etodos que implementamos fueron 2. La norma 2 de distancia a
los centros de masas de cada d\'igito y el m\'as repetido de los 100 vecinos m\'as cercanos al punto.

El m\'etodo de la norma dos en la figura \ref{fig:HRN2}, es sorprendentemente bueno. Comparado con el m\'etodo de
los vecinos m\'as cercanos, es consistentemente mejor para cualquier $k<100$, que son los m\'as
interesantes, ya que idealmente queremos detectar utilizando la menor cantidad de informaci\'on posbile. Una ventaja
de este m\'etodo, es que solamente hay que comprar contra 10 valores (cada media de d\'igito), lo cual lo hace enormemente
m\'as rapido que el otro m\'etodo.

Notablemente, la idea de conseguir el m\'as repetido de los 100 vecinos m\'as cercanos en \ref{fig:HRVEC}
parece tener sentido, pero experimentalmente no es para nada buena para $k$ evaluados, en varios aspectos.
No provee una mejor detecci\'on, de hecho es consistentemente peor para cualquier $k$ y su runtime adem\'as es enorme,
ya que debe hacer norma 2 para todos los otros puntos que deseemos considerar, pueden andar en el orden de miles tal vez.
Claramente si se desea usar este mecanismo, habr\'ia que pensar en mejores estructuras
de datos que puedan simplificar el calculo de distancia. Se podria pensar en un algoritmo
que haga clustering de datos, que no sea solamente agrupar por d\'igito, para poder reducir
el espacio de busqueda.


\subsection{Detecci\'on por d\'igito}
Calcular el Hitrate por d\'igito se nos ocurri\'o tarde, pero son resultados
sumamente interesantes. De la figura \ref{fig:HRD30kcv-n2} a \ref{fig:HRD30kcv-dist100}
podemos apreciar considerablemente que, dentro de
la detecci\'on de d\'igitos, hay algunos que son m\'as dificiles que otros.

Por un lado, el d\'igito 1 es el m\'as facil de reconocer. Nuestra hip\'otesis
es que no es normal dibujarlo con m\'as de un trazo, y eso simplifica mucho la
descomposici\'on en componentes principales, al tener que capturar unicamente ese
fen\'omeno.

Por el otro, el d\'igito 5 es much\'isimo m\'as complicado de reconocer, teniendo
hitrates de una fracci\'on de los otros d\'igitos, incluso con m\'as componentes
principales. Esto se debe al fen\'omeno de que es a veces sut\'il la diferencia
entre los d\'igitos como 2 y 8.

En las figuras \ref{fig:HM30kcv-k5} hasta \ref{fig:HM30kcv-k50}, podemos apreciar
en un formato HeatMap como varian los aciertos. Usamos norma 2 para la detecci\'on ac\'a
ya que antes pudimos apreciar que era m\'etodo m\'as efectivo. Vemos que cuando tomamos pocas
columnas, la diagonal del heatmap (que es el acierto) esta muy difusa, y vemos tambi\'en
otros puntos fuera de la diagonal que tienen un valor bastante alto. Estos son los d\'igitos
que se confunden. Podemos apreciar, por ejemplo, en la figura \ref{fig:HM30kcv-k5}, que
los d\'gitos que m\'as se pueden confundir, con 5 columnas, son el 8 por el 2 y el 4 por el 1.

Seria interesante poder combinar este mecanismo de detecci\'on con algun otro basado en un
criterio diferente, pero que tenga m\'as poder clasificar entre los d\'igitos problematicos
previamente comentados.


\subsection{Cantidad de componentes principales a tomar para la comparaci\'on}
Un punto clave de este trabajo fue la determinaci\'on de la cantidad de valores que debemos
utilizar para comparar im\'agenes. En todas las figuras pudimos observar, en mayor o menor
grado, un crecimiento del hitrate con respecto a la cantidad de componentes principales tomados.
Obviamente, tomar mas elementos lleva a una mejor caracterizaci\'on', pero se vuelve un problema
de que uno esta b\'asicamente identificando una imagen pixel por pixel. Lo interesante de un
sistema de reconocimiento de im\'agenes es tener una alto grado de detecci\'on utilizando la menor
cantidad de recursos, espaciales y temporales.


Se puede apreciar en las figuras \ref{fig:HM30kcv-k5} hasta \ref{fig:HM30kcv-k100},
que hay una diferencia considerable en la elecci\'on del m\'etodo usado,
pero sobre todo en la cantidad de componentes principales elegidos. Sin embargo, a partir de ciertos valores
ya el crecimiento del hitrate es muy lento ($K=50$ por ejemplo). Esto en realidad importa poco
si la cantidad de im\'agenes que tenemos que detectar no es grande, la diferencia de cantidad
de operaciones que hay entre $K=50$ y $K=150$ no es enorme. Dicho esto, hay que tener en cuenta que
en sistemas \textit{real-time} esto puede ser excesivamente costoso. Seg\'un nuestros experimentos,
se deberia hacer un analisis de los requerimientos de forma cuidadosa y ver si el \textit{trade-off} tiempo/precisi\'on
es aceptable o no.
