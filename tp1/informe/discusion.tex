\section{Discusi\'on}

En un primer momento, el \'enfasis estuvo puesto en hacer funcionar el m\'etodo 
de Newton.
Este m\'etodo fue el primero en ser planteado e implementado debido a la
importancia recibida tanto en clase como en el material de lectura consultado.
Consecuentemente con lo esperado el m\'etodo de Newton funcion\'o exitosamente
sin la necesidad de mayores incursiones en el c\'odigo original o en las cuentas
realizadas previamente.

Luego, investigando por los otros m\'etodos de c\'alculo de ra\'ices; 
el primero que se present\'o fue la aproximaci\'on num\'erica de Newton, 
el m\'etodo de la secante. Las complejidades encontradas para el desarrollo de 
esta funci\'on fueron la facilidad con la cual el denominador tiende a cero, 
convirtiendo la cuenta en NaN (Not a Number) rapidamente. Elegimos para 
reemplazarlo el m\'etodo de Regula Falsi.

Un detalle interesante es que entre m\'as decimales se emplean, m\'as rapido 
converge el m\'etodo de Newton. Esto tiene sentido debido a que las 
aproximaciones num\'ericas son cada vez mejores. Tambi\'en se observa que para 
valores de precision muy bajos (menos de 14 decimales binarios en promedio), 
Newton directamente diverge. 
Este fenomeno se debe a que la precisi\'on del epsilon elegido 
($\epsilon = 10^{-4}$) es apenas mayor que $2^{-14}$. 
Esto produce que queden pocos bits para la que ser\'ia parte entera dentro de 
la mantisa. Como esos bits se tienen que compartir entre parte entera y parte 
decimal, no se tienen exactamente 14 bits para los decimales, sino que a medida 
que se va a agrandando el n\'umero, la densidad de los reales representables 
disminuye dr\'asticamente.

Fue de inter\'es trazar las curvas de aproximaci\'on de las variables. 
Esto es de alta relevancia ya que se puede ver realmente el impacto que se tiene
al utilizar mayor cantidad de d\'igitos. Lo m\'as notable fue visuzalizar la 
aproximaci\'on asint\'otica a los valores ``verdaderos'' de las variables, 
y como las ganancias marginales a partir de, en promedio, 22 decimales, 
se vuelven despreciables. Nuestra hip\'otesis es que con esa cantidad de 
d\'igitos, la mantisa puede ser suficientemente bien ajustada a tal punto que puede 
ser representada bien la parte entera y la parte decimal minimice los errores de 
operaci\'on. Por ejemplo, en el caso 1, como $\beta = 9$, se necesitan al menos 
5 bits para representar la parte entera. Luego, todos los restantes seran usados 
para decimales. Si hubiera 15 bits en total, quedar\'ian 10 bits para la parte 
decimal. $2^{-10}<10{-4}<2^{-9}$, luego la parte decimal, va a tener una
precisi\'on cercana a $\frac{1}{1000}$, en caso promedio. 
Sin embargo no alcanza a converger a los valores originales que generaron los 
datos por la acumulacion de errores en, por ejemplo, las funciones
de sumatoria, logaritmos y potenciaci\'on. Esta \'ultima es especialmente 
sensible a los errores en el exponente, y cuando se la procesa de forma 
acumulada (como en una sumatoria), se puede llegar a arrastrar un error
considerable. Como la funci\'on que estima
$\beta$ utiliza sucesivas sumatorias y potencia, podemos ver que ese es un punto 
importante a la hora de intentar minimizar el error de la estimaci\'on.

Debido a discrepancias de convergencia en los casos 1 y 2 utilizando el m\'etodo
de Regla falsa, se procedi\'o a buscar una mejora en este que nos permita una mejor
tasa de convergencia y un ajuste en los valores. Esto fue logrado mediante la
implementaci\'on del algoritmo de Illinois. Gracias a esta mejora se alcanzaron 
mejores curvas de valores.
